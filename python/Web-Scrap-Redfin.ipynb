{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Go to [this](https://www.redfin.com/zipcode/98052) website. It is the house of your area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Use beautifulsoup to pull the pokemon's name, number, and fighting type out of the elements in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import urllib, locale, requests, sys, time\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "soup = BeautifulSoup(urllib.request.urlopen('http://www.serebii.net/diamondpearl/pokemon.shtml').read(), \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Scraping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# Scraping functions\n",
    "# ----------------------\n",
    "\n",
    "def get_listing_pages_count(url, headers):\n",
    "  session = requests.Session()\n",
    "  req = session.get(url, headers = headers)\n",
    "  soup = BeautifulSoup(req.text, \"lxml\")\n",
    "  t = soup.find('span', {'class':'pageText'}).text\n",
    "  p_last = int(t.split(' ')[4])\n",
    "  return p_last\n",
    "\n",
    "def get_listing_pages_urls(url, p_last):\n",
    "  urls = [url]\n",
    "  for page in range(p_last + 1)[2:]:\n",
    "    u1 = url + '/page-' + str(page)\n",
    "    urls.append(u1)\n",
    "  return urls\n",
    "\n",
    "def get_listing_ids(urls, headers):\n",
    "  id_list = []\n",
    "  for url in urls:\n",
    "    time.sleep(np.random.uniform(.25, 1.0))\n",
    "    session = requests.Session()\n",
    "    req = session.get(url, headers = headers)\n",
    "    soup = BeautifulSoup(req.text, \"lxml\")\n",
    "    home_list = soup.find_all('a', {'class':'ViewDetailsButtonWrapper'})\n",
    "    for h in home_list:\n",
    "      id_list.append(h['href'])\n",
    "  return id_list\n",
    "\n",
    "def get_listing_details(id_value, headers):\n",
    "  time.sleep(np.random.uniform(0.25,1.0))\n",
    "  url_id = 'https://www.redfin.com' + id_value\n",
    "  session = requests.Session()\n",
    "  req = session.get(url_id, headers = headers)\n",
    "  soup = BeautifulSoup(req.text, \"lxml\")\n",
    "  stats   = soup.find('div', {'class':'top-stats'})\n",
    "  price   = stats.find('span', {'itemprop':'price'          })['content']\n",
    "  address = stats.find('span', {'itemprop':'streetAddress'  })['title'  ]\n",
    "  city    = stats.find('span', {'itemprop':'addressLocality'}).text.strip(', ')\n",
    "  state   = stats.find('span', {'itemprop':'addressRegion'  }).text\n",
    "  zipcode = stats.find('span', {'itemprop':'postalCode'     }).text\n",
    "  lat     = stats.find('meta', {'itemprop':'latitude'       })['content']\n",
    "  lon     = stats.find('meta', {'itemprop':'longitude'      })['content']\n",
    "  price = int(price)\n",
    "  lat   = float(lat)\n",
    "  lon   = float(lon)\n",
    "  facts = soup.find('div', {'class':'basic-info'})\n",
    "  if facts == None:\n",
    "    cat   = ''\n",
    "    beds  = ''\n",
    "    baths = ''\n",
    "    sqft  = ''\n",
    "    usqft = ''\n",
    "    lot   = ''\n",
    "    year  = ''\n",
    "    reno  = ''\n",
    "  else:\n",
    "    rows = facts.find_all('div', {'class':'table-row'})\n",
    "    ldict = {}\n",
    "    for r in rows:\n",
    "      t = r.find('div' ).string.replace(u'\\u2014', '')\n",
    "      ldict[r.find('span').string] = t\n",
    "    cat   = ldict['Style'] if 'Style' in ldict else ''\n",
    "    beds  = ldict['Beds' ] if 'Beds'  in ldict else ''\n",
    "    baths = ldict['Baths'] if 'Baths' in ldict else ''\n",
    "    sqft  = ldict['Finished Sq. Ft.'  ] if 'Finished Sq. Ft.'   in ldict else ''\n",
    "    usqft = ldict['Unfinished Sq. Ft.'] if 'Unfinished Sq. Ft.' in ldict else ''\n",
    "    lot   = ldict['Lot Size'      ] if 'Lot Size'       in ldict else ''\n",
    "    year  = ldict['Year Built'    ] if 'Year Built'     in ldict else ''\n",
    "    reno  = ldict['Year Renovated'] if 'Year Renovated' in ldict else ''\n",
    "    sqft  = sqft.replace(',','')\n",
    "    usqft = usqft.replace(',','')\n",
    "    lot   = acre_converter(lot)\n",
    "  return (price, address, city, state, zipcode, lat, lon,\n",
    "          cat, beds, baths, sqft, usqft, lot, year, reno)\n",
    "\n",
    "def acre_converter(value):\n",
    "  if 'Sq. Ft.' in value:\n",
    "    v = value.strip(' Sq. Ft.').replace(',','')\n",
    "    return round(float(v)/43560.0, 2)\n",
    "  elif 'Acres' in value:\n",
    "    v = value.strip(' Acres')\n",
    "    return round(float(v),2)\n",
    "  elif ',' in value:\n",
    "    v = value.replace(',','')\n",
    "    return round(float(v)/43560.0, 2)\n",
    "  else:\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-2e647c6bf87b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mid_value\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mid_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m   \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid_value\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_listing_details\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m   \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-dbad026cf538>\u001b[0m in \u001b[0;36mget_listing_details\u001b[1;34m(id_value, headers)\u001b[0m\n\u001b[0;32m     42\u001b[0m   \u001b[0mstate\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'itemprop'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'addressRegion'\u001b[0m  \u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m   \u001b[0mzipcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'itemprop'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'postalCode'\u001b[0m     \u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m   \u001b[0mlat\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'meta'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'itemprop'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'latitude'\u001b[0m       \u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m   \u001b[0mlon\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'meta'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'itemprop'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'longitude'\u001b[0m      \u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m   \u001b[0mprice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# zipcode = '98052'\n",
    "# url = 'https://www.redfin.com/zipcode/' + zipcode\n",
    "# url = 'https://www.redfin.com/city/14913/WA/Redmond'\n",
    "# city = 'Redmond'\n",
    "url = 'https://www.redfin.com/city/1387/WA/Bellevue'\n",
    "city = 'Bellevue'\n",
    "\n",
    "# Add custom HTTP headers\n",
    "ua1 = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) '\n",
    "ua2 = 'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "ua3 = 'Chrome/59.0.3071.115 Safari/537.36'\n",
    "ac1 = 'text/html,application/xhtml+xml,application/xml;'\n",
    "ac2 = 'q=0.9,image/webp,image/apng,*/*;q=0.8'\n",
    "headers = {'User-Agent' : ua1 + ua2 + ua3,\n",
    "           'Accept'     : ac1 + ac2}\n",
    "\n",
    "p_last  = get_listing_pages_count(url, headers)\n",
    "urls    = get_listing_pages_urls(url, p_last)\n",
    "id_list = get_listing_ids(urls, headers)\n",
    "\n",
    "results = {}\n",
    "for id_value in id_list:\n",
    "  results[id_value] = get_listing_details(id_value, headers)\n",
    "  time.sleep(np.random.uniform(0.25,1.0))\n",
    "\n",
    "df = pd.DataFrame.from_dict(results, orient = 'index').reset_index()\n",
    "df.rename(columns = {'index' : 'ID',\n",
    "                     0  : 'Price',\n",
    "                     1  : 'Address',\n",
    "                     2  : 'City',\n",
    "                     3  : 'State',\n",
    "                     4  : 'ZipCode',\n",
    "                     5  : 'Latitude',\n",
    "                     6  : 'Longitude',\n",
    "                     7  : 'Category',\n",
    "                     8  : 'Beds',\n",
    "                     9  : 'Baths',\n",
    "                     10 : 'SqFt',\n",
    "                     11 : 'UnSqFt',\n",
    "                     12 : 'LotSize',\n",
    "                     13 : 'YearBuilt',\n",
    "                     14 : 'YearReno'},\n",
    "          inplace = True)\n",
    "\n",
    "# df.to_csv('redfin_' + zipcode + '.csv', encoding = 'utf-8')\n",
    "\n",
    "df.to_csv('redfin_' + city + '.csv', encoding = 'utf-8')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
